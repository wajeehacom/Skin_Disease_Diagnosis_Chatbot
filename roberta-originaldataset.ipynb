{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10786412,"sourceType":"datasetVersion","datasetId":6693643}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Initialize the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load dataset\ndf = pd.read_csv('/kaggle/input/finalized-robertadataset/FinalizedSkinDiseaseDataset.csv', on_bad_lines=\"skip\")\ndf.columns = df.columns.str.strip()\ndf = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n\ndf.columns = ['Disease name', 'Text']  # Rename columns\ndf['Disease name'] = df['Disease name'].str.strip('\"')\ndf['Text'] = df['Text'].str.strip('\"')\ndf['Disease name'] = df['Disease name'].astype('category')\n\n# Label mapping\nlabel_map = {label: i for i, label in enumerate(df['Disease name'].unique())}\ndf['Disease name'] = df['Disease name'].map(label_map)\n\n# Train/test split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df['Text'], df['Disease name'], test_size=0.2, random_state=42\n)\n\n# Use RoBERTa Model\nmodel_name = \"roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntrain_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=256)\nval_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=256)\n\n# Compute class weights\nclass_weights = compute_class_weight(\n    'balanced', classes=np.array(df['Disease name'].unique()), y=df['Disease name']\n)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n\n# Dataset class\nclass SkinDiseaseDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels.iloc[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# DataLoader\ntrain_loader = DataLoader(SkinDiseaseDataset(train_encodings, train_labels), batch_size=16, shuffle=True, num_workers=4)\nval_loader = DataLoader(SkinDiseaseDataset(val_encodings, val_labels), batch_size=16, num_workers=4)\n\n# Load RoBERTa model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_map))\nmodel.to(device)\n\n# Optimizer and Scheduler\noptimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8, weight_decay=0.01)\nnum_train_steps = len(train_loader) * 20\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*num_train_steps, num_training_steps=num_train_steps)\n\n# Training function\ndef train_epoch(model, data_loader, optimizer, scheduler, device):\n    model.train()\n    total_loss, correct_preds, total_preds = 0, 0, 0\n    for batch in data_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n        preds = torch.argmax(outputs.logits, dim=-1)\n        correct_preds += (preds == batch['labels']).sum().item()\n        total_preds += len(batch['labels'])\n    return total_loss / len(data_loader), correct_preds / total_preds\n\n# Evaluation function\ndef evaluate(model, data_loader, device):\n    model.eval()\n    total_preds, total_labels = [], []\n    with torch.no_grad():\n        for batch in data_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            preds = torch.argmax(outputs.logits, dim=-1)\n            total_preds.extend(preds.cpu().numpy())\n            total_labels.extend(batch['labels'].cpu().numpy())\n    return (\n        accuracy_score(total_labels, total_preds),\n        precision_score(total_labels, total_preds, average='weighted'),\n        recall_score(total_labels, total_preds, average='weighted'),\n        f1_score(total_labels, total_preds, average='weighted')\n    )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T18:35:15.316016Z","iopub.execute_input":"2025-02-18T18:35:15.316213Z","iopub.status.idle":"2025-02-18T18:35:40.629419Z","shell.execute_reply.started":"2025-02-18T18:35:15.316180Z","shell.execute_reply":"2025-02-18T18:35:40.628511Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd58b16fa704dda884bfc755c1c6a2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e85dd83116d74e47a826256f03b69ae0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a906c78914b46b78b59d1d177593548"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd45f59577740eba57f99d27031f626"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff2052d653754c5b8c615dad552be59f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47f8195df0ed44d9998881b73a0662ca"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Training loop with early stopping\nbest_val_accuracy, patience, epochs_without_improvement = 0, 3, 0\nepochs = 50\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, scheduler, device)\n    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n    val_accuracy, precision, recall, f1 = evaluate(model, val_loader, device)\n    print(f\"Validation Accuracy: {val_accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n    print('-' * 50)\n    \n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        epochs_without_improvement = 0\n        model.save_pretrained('./roberta_skin_disease_model')\n        tokenizer.save_pretrained('./roberta_skin_disease_model')\n    else:\n        epochs_without_improvement += 1\n    \n    if epochs_without_improvement >= patience:\n        print(\"Early stopping triggered\")\n        break\n\n# Final save\nmodel.save_pretrained('./roberta_skin_disease_model')\ntokenizer.save_pretrained('./roberta_skin_disease_model')\nprint(\"Fine-tuning completed and RoBERTa model saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T18:37:52.161182Z","iopub.execute_input":"2025-02-18T18:37:52.161552Z","iopub.status.idle":"2025-02-18T18:38:40.212693Z","shell.execute_reply.started":"2025-02-18T18:37:52.161527Z","shell.execute_reply":"2025-02-18T18:38:40.211575Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\nTrain Loss: 0.1712, Train Accuracy: 0.9748\nValidation Accuracy: 0.8994, Precision: 0.9134, Recall: 0.8994, F1-Score: 0.9015\n--------------------------------------------------\nEpoch 2/50\nTrain Loss: 0.1556, Train Accuracy: 0.9764\nValidation Accuracy: 0.8868, Precision: 0.9221, Recall: 0.8868, F1-Score: 0.8876\n--------------------------------------------------\nEpoch 3/50\nTrain Loss: 0.1394, Train Accuracy: 0.9858\nValidation Accuracy: 0.8868, Precision: 0.9037, Recall: 0.8868, F1-Score: 0.8849\n--------------------------------------------------\nEpoch 4/50\nTrain Loss: 0.1300, Train Accuracy: 0.9843\nValidation Accuracy: 0.8931, Precision: 0.8976, Recall: 0.8931, F1-Score: 0.8928\n--------------------------------------------------\nEarly stopping triggered\nFine-tuning completed and RoBERTa model saved.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T18:39:36.758154Z","iopub.execute_input":"2025-02-18T18:39:36.758506Z","iopub.status.idle":"2025-02-18T18:39:36.764139Z","shell.execute_reply.started":"2025-02-18T18:39:36.758478Z","shell.execute_reply":"2025-02-18T18:39:36.763291Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import shutil\n\n# Create a zip file of the model directory\nshutil.make_archive('/kaggle/working/roberta_skin_disease_model', 'zip', '/kaggle/working', 'roberta_skin_disease_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T18:39:39.729054Z","iopub.execute_input":"2025-02-18T18:39:39.729485Z","iopub.status.idle":"2025-02-18T18:40:12.338919Z","shell.execute_reply.started":"2025-02-18T18:39:39.729447Z","shell.execute_reply":"2025-02-18T18:40:12.338078Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/roberta_skin_disease_model.zip'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink('roberta_skin_disease_model.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T18:40:42.271334Z","iopub.execute_input":"2025-02-18T18:40:42.271637Z","iopub.status.idle":"2025-02-18T18:40:42.276907Z","shell.execute_reply.started":"2025-02-18T18:40:42.271616Z","shell.execute_reply":"2025-02-18T18:40:42.276237Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/roberta_skin_disease_model.zip","text/html":"<a href='roberta_skin_disease_model.zip' target='_blank'>roberta_skin_disease_model.zip</a><br>"},"metadata":{}}],"execution_count":7}]}