{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPkLySl8YvwtLpmCBgsqIPR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1oP90XdFYCEt","executionInfo":{"status":"ok","timestamp":1735877967246,"user_tz":480,"elapsed":4666,"user":{"displayName":"wajeeha XYZ","userId":"02510792856806663883"}},"outputId":"cb62a769-85ad-416a-fd3f-d947dcca07ed"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Provide the path to your dataset in Google Drive\n","file_path = '/content/drive/My Drive/textual_dataset/cleaned_skindiseasesdataset.csv'\n","\n","# Load the CSV dataset with a specified encoding\n","dataset = pd.read_csv(file_path, encoding='ISO-8859-1')\n","\n","# Display the dataset structure\n","print(dataset.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eqbJDh4oYj7D","executionInfo":{"status":"ok","timestamp":1735878075189,"user_tz":480,"elapsed":1152,"user":{"displayName":"wajeeha XYZ","userId":"02510792856806663883"}},"outputId":"f2a6a26d-535c-4c62-a3c4-7daa71c1dd42"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["              Output                                              Input\n","0           Vitiligo  \"I've had these light patches on my neck and f...\n","1           Vitiligo                                 \"I've patchy skin\"\n","2            Scabies  \"Doctor, I've noticed these small, red bumps o...\n","3           Vitiligo  \"Doctor, I noticed a pale patch around my knee...\n","4  Hives (Urticaria)  Hives, also known as urticaria, typically pres...\n"]}]},{"cell_type":"code","source":["ffrom sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset\n","from transformers import GPT2Tokenizer\n","\n","# Initialize the tokenizer\n","model_name = 'gpt2'\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token  # Set the pad token to be the same as the eos token\n","\n","# Tokenization function\n","def tokenize_function(inputs, outputs):\n","    input_encodings = tokenizer(inputs, truncation=True, padding='max_length', max_length=256)\n","    output_encodings = tokenizer(outputs, truncation=True, padding='max_length', max_length=256)\n","    return input_encodings, output_encodings\n","\n","# Tokenize the dataset\n","input_texts = dataset['Input'].tolist()\n","output_texts = dataset['Output'].tolist()\n","\n","tokenized_inputs, tokenized_outputs = tokenize_function(input_texts, output_texts)\n","\n","# Verify the shape of tokenized texts\n","print(f\"Number of tokenized input texts: {len(tokenized_inputs['input_ids'])}\")\n","print(f\"Number of tokenized output texts: {len(tokenized_outputs['input_ids'])}\")\n","\n","# Determine the number of unique labels\n","unique_labels = list(set(dataset['Output'].str.strip()))\n","num_labels = len(unique_labels)\n","\n","# Create a mapping from label to ID\n","label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n","\n","# Prepare the dataset for training\n","def format_labels(labels):\n","    return [label_to_id[label.strip()] for label in labels]\n","\n","# Create labels\n","formatted_labels = format_labels(dataset['Output'].tolist())\n","\n","# Create a custom dataset class\n","class CustomDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Create a train-validation split\n","train_texts, val_texts, train_labels, val_labels = train_test_split(\n","    tokenized_inputs['input_ids'], formatted_labels, test_size=0.1\n",")\n","\n","# Create datasets\n","train_encodings = {\n","    'input_ids': train_texts,\n","    'attention_mask': tokenized_inputs['attention_mask'][:len(train_texts)]\n","}\n","val_encodings = {\n","    'input_ids': val_texts,\n","    'attention_mask': tokenized_inputs['attention_mask'][len(train_texts):]\n","}\n","\n","train_dataset = CustomDataset(encodings=train_encodings, labels=train_labels)\n","val_dataset = CustomDataset(encodings=val_encodings, labels=val_labels)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tmIFZDIHYlw_","executionInfo":{"status":"ok","timestamp":1735878152134,"user_tz":480,"elapsed":2029,"user":{"displayName":"wajeeha XYZ","userId":"02510792856806663883"}},"outputId":"9662e701-74d2-4cd7-85c3-6d191ab304ce"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of tokenized input texts: 483\n","Number of tokenized output texts: 483\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset\n","\n","# Determine the number of unique labels\n","unique_labels = list(set(dataset['Output'].str.strip()))\n","num_labels = len(unique_labels)\n","\n","# Create a mapping from label to ID\n","label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n","\n","# Prepare the dataset for training\n","def format_labels(labels):\n","    return [label_to_id[label.strip()] for label in labels]\n","\n","# Create labels\n","formatted_labels = format_labels(dataset['Output'].tolist())\n","\n","# Create a custom dataset class\n","class CustomDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n"],"metadata":{"id":"8jhEYDPQYnki","executionInfo":{"status":"ok","timestamp":1735878363589,"user_tz":480,"elapsed":656,"user":{"displayName":"wajeeha XYZ","userId":"02510792856806663883"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Create a train-validation split\n","train_texts, val_texts, train_labels, val_labels = train_test_split(\n","    tokenized_inputs['input_ids'], formatted_labels, test_size=0.1\n",")\n","\n","# Create datasets\n","train_encodings = {\n","    'input_ids': train_texts,\n","    'attention_mask': tokenized_inputs['attention_mask'][:len(train_texts)]\n","}\n","val_encodings = {\n","    'input_ids': val_texts,\n","    'attention_mask': tokenized_inputs['attention_mask'][len(train_texts):]\n","}\n","\n","train_dataset = CustomDataset(encodings=train_encodings, labels=train_labels)\n","val_dataset = CustomDataset(encodings=val_encodings, labels=val_labels)\n"],"metadata":{"id":"KVJ9lxGTYrQz","executionInfo":{"status":"ok","timestamp":1735878427608,"user_tz":480,"elapsed":573,"user":{"displayName":"wajeeha XYZ","userId":"02510792856806663883"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2ForSequenceClassification\n","\n","# Define the model and set padding token\n","model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n","model.config.pad_token_id = tokenizer.pad_token_id\n","\n","# Freeze all layers except the last one\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# Unfreeze the last transformer layer\n","for param in model.transformer.h[-1].parameters():  # Unfreeze the last transformer block\n","    param.requires_grad = True\n","\n","# Unfreeze the classification head (score layer)\n","for param in model.score.parameters():  # Unfreeze the score layer\n","    param.requires_grad = True\n","\n","# Verify that only the last layers are unfrozen\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        print(f\"Training parameter: {name}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JWBLKEmQYujx","executionInfo":{"status":"ok","timestamp":1735878523811,"user_tz":480,"elapsed":2494,"user":{"displayName":"wajeeha XYZ","userId":"02510792856806663883"}},"outputId":"368a03d3-0250-4156-ebe8-51ac54fafb32"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Training parameter: transformer.h.11.ln_1.weight\n","Training parameter: transformer.h.11.ln_1.bias\n","Training parameter: transformer.h.11.attn.c_attn.weight\n","Training parameter: transformer.h.11.attn.c_attn.bias\n","Training parameter: transformer.h.11.attn.c_proj.weight\n","Training parameter: transformer.h.11.attn.c_proj.bias\n","Training parameter: transformer.h.11.ln_2.weight\n","Training parameter: transformer.h.11.ln_2.bias\n","Training parameter: transformer.h.11.mlp.c_fc.weight\n","Training parameter: transformer.h.11.mlp.c_fc.bias\n","Training parameter: transformer.h.11.mlp.c_proj.weight\n","Training parameter: transformer.h.11.mlp.c_proj.bias\n","Training parameter: score.weight\n"]}]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","# Training arguments\n","training_args = TrainingArguments(\n","    output_dir='/content/drive/My Drive/gpt_fine_tune',\n","    evaluation_strategy='epoch',\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,  # Experiment with smaller batch size\n","    num_train_epochs=200,  # Set the number of epochs to 200\n","    logging_dir='/content/drive/My Drive/logs_gpt',\n","    report_to='none',\n","    logging_steps=10  # Log every 10 steps\n",")\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"goswL3WxYwt9","executionInfo":{"status":"ok","timestamp":1735878580320,"user_tz":480,"elapsed":3506,"user":{"displayName":"wajeeha XYZ","userId":"02510792856806663883"}},"outputId":"eda7d8cc-cafe-4b4a-80bf-1efe68a04823"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Train the model\n","trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"xFsJwnBGYzOp","executionInfo":{"status":"ok","timestamp":1735880432151,"user_tz":480,"elapsed":1839291,"user":{"displayName":"wajeeha XYZ","userId":"02510792856806663883"}},"outputId":"2bd60364-7b5e-456d-9d01-c9d88fd12962"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11000/11000 30:36, Epoch 200/200]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>4.931000</td>\n","      <td>4.343917</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.129100</td>\n","      <td>3.052096</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.996700</td>\n","      <td>2.914535</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.903500</td>\n","      <td>2.852580</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>2.605000</td>\n","      <td>2.820974</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>2.601800</td>\n","      <td>2.792178</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>2.658300</td>\n","      <td>2.759446</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>2.636800</td>\n","      <td>2.738565</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>2.567100</td>\n","      <td>2.722962</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>2.622100</td>\n","      <td>2.701558</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>2.540000</td>\n","      <td>2.682728</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>2.456400</td>\n","      <td>2.657609</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>2.311200</td>\n","      <td>2.624295</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>2.244900</td>\n","      <td>2.594684</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>2.448000</td>\n","      <td>2.575598</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>2.339500</td>\n","      <td>2.539829</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>2.258000</td>\n","      <td>2.522818</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>2.306800</td>\n","      <td>2.490870</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>2.167800</td>\n","      <td>2.464338</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.063000</td>\n","      <td>2.416879</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>2.016500</td>\n","      <td>2.379878</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.980600</td>\n","      <td>2.347358</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>1.966100</td>\n","      <td>2.310855</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>1.917000</td>\n","      <td>2.274211</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>1.784900</td>\n","      <td>2.233255</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.458500</td>\n","      <td>2.185159</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>1.635700</td>\n","      <td>2.159002</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>1.660000</td>\n","      <td>2.100727</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>1.515000</td>\n","      <td>2.058915</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.514500</td>\n","      <td>2.020344</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>1.289300</td>\n","      <td>1.933200</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>1.497000</td>\n","      <td>1.912171</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>1.350500</td>\n","      <td>1.831319</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>1.539300</td>\n","      <td>1.800155</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>1.024000</td>\n","      <td>1.782623</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>1.029700</td>\n","      <td>1.756230</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>1.090000</td>\n","      <td>1.700756</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>1.104600</td>\n","      <td>1.712702</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>1.140700</td>\n","      <td>1.650069</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.937500</td>\n","      <td>1.631251</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>1.011100</td>\n","      <td>1.624823</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>1.188600</td>\n","      <td>1.636211</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>1.254100</td>\n","      <td>1.583549</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.959100</td>\n","      <td>1.603365</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.832800</td>\n","      <td>1.538151</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.914600</td>\n","      <td>1.477413</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.735300</td>\n","      <td>1.519199</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.730500</td>\n","      <td>1.521984</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.790100</td>\n","      <td>1.455835</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.742000</td>\n","      <td>1.506590</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.917100</td>\n","      <td>1.484422</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.734000</td>\n","      <td>1.471736</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.926400</td>\n","      <td>1.484527</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.608300</td>\n","      <td>1.490280</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.536300</td>\n","      <td>1.477597</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.600500</td>\n","      <td>1.500644</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.489300</td>\n","      <td>1.472417</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.859500</td>\n","      <td>1.477892</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.582000</td>\n","      <td>1.465612</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.503800</td>\n","      <td>1.447553</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.620600</td>\n","      <td>1.417333</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.531700</td>\n","      <td>1.395238</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.664100</td>\n","      <td>1.375911</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.618000</td>\n","      <td>1.383701</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.449000</td>\n","      <td>1.440553</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.599600</td>\n","      <td>1.416764</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.541100</td>\n","      <td>1.396341</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.483500</td>\n","      <td>1.363707</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.549000</td>\n","      <td>1.378285</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.634000</td>\n","      <td>1.401479</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.501000</td>\n","      <td>1.339398</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.502700</td>\n","      <td>1.395817</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.247800</td>\n","      <td>1.348832</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.547200</td>\n","      <td>1.391207</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.338500</td>\n","      <td>1.385990</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.526400</td>\n","      <td>1.387332</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.335100</td>\n","      <td>1.374330</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.435000</td>\n","      <td>1.451168</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.444400</td>\n","      <td>1.376493</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.449900</td>\n","      <td>1.356483</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.363400</td>\n","      <td>1.408504</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.357700</td>\n","      <td>1.416440</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.431700</td>\n","      <td>1.428818</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.283000</td>\n","      <td>1.383966</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.409500</td>\n","      <td>1.374070</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.397100</td>\n","      <td>1.385423</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.395300</td>\n","      <td>1.412624</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.295800</td>\n","      <td>1.406890</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.275800</td>\n","      <td>1.367658</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.396000</td>\n","      <td>1.379376</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.332800</td>\n","      <td>1.368087</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.332500</td>\n","      <td>1.385969</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.406800</td>\n","      <td>1.346983</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.247100</td>\n","      <td>1.403422</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.286800</td>\n","      <td>1.360280</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.382400</td>\n","      <td>1.388990</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.265900</td>\n","      <td>1.385228</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.359300</td>\n","      <td>1.382314</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.387500</td>\n","      <td>1.380594</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.256200</td>\n","      <td>1.388816</td>\n","    </tr>\n","    <tr>\n","      <td>101</td>\n","      <td>0.322800</td>\n","      <td>1.375533</td>\n","    </tr>\n","    <tr>\n","      <td>102</td>\n","      <td>0.329100</td>\n","      <td>1.394779</td>\n","    </tr>\n","    <tr>\n","      <td>103</td>\n","      <td>0.223300</td>\n","      <td>1.374963</td>\n","    </tr>\n","    <tr>\n","      <td>104</td>\n","      <td>0.323100</td>\n","      <td>1.385892</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>0.191100</td>\n","      <td>1.373401</td>\n","    </tr>\n","    <tr>\n","      <td>106</td>\n","      <td>0.347300</td>\n","      <td>1.364209</td>\n","    </tr>\n","    <tr>\n","      <td>107</td>\n","      <td>0.233800</td>\n","      <td>1.377624</td>\n","    </tr>\n","    <tr>\n","      <td>108</td>\n","      <td>0.237500</td>\n","      <td>1.370162</td>\n","    </tr>\n","    <tr>\n","      <td>109</td>\n","      <td>0.210000</td>\n","      <td>1.387580</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.314900</td>\n","      <td>1.352153</td>\n","    </tr>\n","    <tr>\n","      <td>111</td>\n","      <td>0.276000</td>\n","      <td>1.363060</td>\n","    </tr>\n","    <tr>\n","      <td>112</td>\n","      <td>0.177700</td>\n","      <td>1.372555</td>\n","    </tr>\n","    <tr>\n","      <td>113</td>\n","      <td>0.391900</td>\n","      <td>1.358014</td>\n","    </tr>\n","    <tr>\n","      <td>114</td>\n","      <td>0.124800</td>\n","      <td>1.370269</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>0.223600</td>\n","      <td>1.412816</td>\n","    </tr>\n","    <tr>\n","      <td>116</td>\n","      <td>0.315500</td>\n","      <td>1.409502</td>\n","    </tr>\n","    <tr>\n","      <td>117</td>\n","      <td>0.170000</td>\n","      <td>1.380956</td>\n","    </tr>\n","    <tr>\n","      <td>118</td>\n","      <td>0.346300</td>\n","      <td>1.367183</td>\n","    </tr>\n","    <tr>\n","      <td>119</td>\n","      <td>0.282900</td>\n","      <td>1.429155</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.192000</td>\n","      <td>1.412521</td>\n","    </tr>\n","    <tr>\n","      <td>121</td>\n","      <td>0.200900</td>\n","      <td>1.405238</td>\n","    </tr>\n","    <tr>\n","      <td>122</td>\n","      <td>0.204900</td>\n","      <td>1.410637</td>\n","    </tr>\n","    <tr>\n","      <td>123</td>\n","      <td>0.243500</td>\n","      <td>1.409271</td>\n","    </tr>\n","    <tr>\n","      <td>124</td>\n","      <td>0.165100</td>\n","      <td>1.414804</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.215600</td>\n","      <td>1.420335</td>\n","    </tr>\n","    <tr>\n","      <td>126</td>\n","      <td>0.371800</td>\n","      <td>1.416711</td>\n","    </tr>\n","    <tr>\n","      <td>127</td>\n","      <td>0.233000</td>\n","      <td>1.425524</td>\n","    </tr>\n","    <tr>\n","      <td>128</td>\n","      <td>0.135500</td>\n","      <td>1.421143</td>\n","    </tr>\n","    <tr>\n","      <td>129</td>\n","      <td>0.168300</td>\n","      <td>1.427480</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.159700</td>\n","      <td>1.456913</td>\n","    </tr>\n","    <tr>\n","      <td>131</td>\n","      <td>0.174300</td>\n","      <td>1.457228</td>\n","    </tr>\n","    <tr>\n","      <td>132</td>\n","      <td>0.211700</td>\n","      <td>1.464092</td>\n","    </tr>\n","    <tr>\n","      <td>133</td>\n","      <td>0.201600</td>\n","      <td>1.423932</td>\n","    </tr>\n","    <tr>\n","      <td>134</td>\n","      <td>0.182300</td>\n","      <td>1.434717</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>0.172800</td>\n","      <td>1.428120</td>\n","    </tr>\n","    <tr>\n","      <td>136</td>\n","      <td>0.258200</td>\n","      <td>1.445769</td>\n","    </tr>\n","    <tr>\n","      <td>137</td>\n","      <td>0.200000</td>\n","      <td>1.455588</td>\n","    </tr>\n","    <tr>\n","      <td>138</td>\n","      <td>0.151200</td>\n","      <td>1.460389</td>\n","    </tr>\n","    <tr>\n","      <td>139</td>\n","      <td>0.178100</td>\n","      <td>1.428048</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.156800</td>\n","      <td>1.429688</td>\n","    </tr>\n","    <tr>\n","      <td>141</td>\n","      <td>0.126900</td>\n","      <td>1.475457</td>\n","    </tr>\n","    <tr>\n","      <td>142</td>\n","      <td>0.105100</td>\n","      <td>1.462327</td>\n","    </tr>\n","    <tr>\n","      <td>143</td>\n","      <td>0.147600</td>\n","      <td>1.469152</td>\n","    </tr>\n","    <tr>\n","      <td>144</td>\n","      <td>0.242400</td>\n","      <td>1.458090</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>0.116400</td>\n","      <td>1.487212</td>\n","    </tr>\n","    <tr>\n","      <td>146</td>\n","      <td>0.195200</td>\n","      <td>1.514873</td>\n","    </tr>\n","    <tr>\n","      <td>147</td>\n","      <td>0.159900</td>\n","      <td>1.511192</td>\n","    </tr>\n","    <tr>\n","      <td>148</td>\n","      <td>0.237100</td>\n","      <td>1.495715</td>\n","    </tr>\n","    <tr>\n","      <td>149</td>\n","      <td>0.158600</td>\n","      <td>1.487476</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.113000</td>\n","      <td>1.488022</td>\n","    </tr>\n","    <tr>\n","      <td>151</td>\n","      <td>0.325700</td>\n","      <td>1.474285</td>\n","    </tr>\n","    <tr>\n","      <td>152</td>\n","      <td>0.152200</td>\n","      <td>1.458663</td>\n","    </tr>\n","    <tr>\n","      <td>153</td>\n","      <td>0.133800</td>\n","      <td>1.449477</td>\n","    </tr>\n","    <tr>\n","      <td>154</td>\n","      <td>0.209000</td>\n","      <td>1.445643</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>0.114600</td>\n","      <td>1.457899</td>\n","    </tr>\n","    <tr>\n","      <td>156</td>\n","      <td>0.112000</td>\n","      <td>1.466822</td>\n","    </tr>\n","    <tr>\n","      <td>157</td>\n","      <td>0.114700</td>\n","      <td>1.478470</td>\n","    </tr>\n","    <tr>\n","      <td>158</td>\n","      <td>0.235600</td>\n","      <td>1.482390</td>\n","    </tr>\n","    <tr>\n","      <td>159</td>\n","      <td>0.140700</td>\n","      <td>1.496152</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.144300</td>\n","      <td>1.475126</td>\n","    </tr>\n","    <tr>\n","      <td>161</td>\n","      <td>0.207200</td>\n","      <td>1.482338</td>\n","    </tr>\n","    <tr>\n","      <td>162</td>\n","      <td>0.228800</td>\n","      <td>1.487222</td>\n","    </tr>\n","    <tr>\n","      <td>163</td>\n","      <td>0.177000</td>\n","      <td>1.496137</td>\n","    </tr>\n","    <tr>\n","      <td>164</td>\n","      <td>0.095100</td>\n","      <td>1.504237</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>0.133400</td>\n","      <td>1.494284</td>\n","    </tr>\n","    <tr>\n","      <td>166</td>\n","      <td>0.205300</td>\n","      <td>1.483305</td>\n","    </tr>\n","    <tr>\n","      <td>167</td>\n","      <td>0.195300</td>\n","      <td>1.485161</td>\n","    </tr>\n","    <tr>\n","      <td>168</td>\n","      <td>0.110000</td>\n","      <td>1.489521</td>\n","    </tr>\n","    <tr>\n","      <td>169</td>\n","      <td>0.084700</td>\n","      <td>1.501615</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.100300</td>\n","      <td>1.484576</td>\n","    </tr>\n","    <tr>\n","      <td>171</td>\n","      <td>0.205000</td>\n","      <td>1.459842</td>\n","    </tr>\n","    <tr>\n","      <td>172</td>\n","      <td>0.132000</td>\n","      <td>1.479244</td>\n","    </tr>\n","    <tr>\n","      <td>173</td>\n","      <td>0.129100</td>\n","      <td>1.487730</td>\n","    </tr>\n","    <tr>\n","      <td>174</td>\n","      <td>0.092600</td>\n","      <td>1.491346</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.158200</td>\n","      <td>1.483883</td>\n","    </tr>\n","    <tr>\n","      <td>176</td>\n","      <td>0.270700</td>\n","      <td>1.480407</td>\n","    </tr>\n","    <tr>\n","      <td>177</td>\n","      <td>0.128900</td>\n","      <td>1.478439</td>\n","    </tr>\n","    <tr>\n","      <td>178</td>\n","      <td>0.160100</td>\n","      <td>1.483215</td>\n","    </tr>\n","    <tr>\n","      <td>179</td>\n","      <td>0.134300</td>\n","      <td>1.495484</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.244100</td>\n","      <td>1.499731</td>\n","    </tr>\n","    <tr>\n","      <td>181</td>\n","      <td>0.176000</td>\n","      <td>1.496451</td>\n","    </tr>\n","    <tr>\n","      <td>182</td>\n","      <td>0.139300</td>\n","      <td>1.505808</td>\n","    </tr>\n","    <tr>\n","      <td>183</td>\n","      <td>0.182200</td>\n","      <td>1.508230</td>\n","    </tr>\n","    <tr>\n","      <td>184</td>\n","      <td>0.110700</td>\n","      <td>1.501858</td>\n","    </tr>\n","    <tr>\n","      <td>185</td>\n","      <td>0.118600</td>\n","      <td>1.503788</td>\n","    </tr>\n","    <tr>\n","      <td>186</td>\n","      <td>0.224600</td>\n","      <td>1.506299</td>\n","    </tr>\n","    <tr>\n","      <td>187</td>\n","      <td>0.086800</td>\n","      <td>1.511417</td>\n","    </tr>\n","    <tr>\n","      <td>188</td>\n","      <td>0.107300</td>\n","      <td>1.510788</td>\n","    </tr>\n","    <tr>\n","      <td>189</td>\n","      <td>0.146900</td>\n","      <td>1.511306</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.153600</td>\n","      <td>1.505794</td>\n","    </tr>\n","    <tr>\n","      <td>191</td>\n","      <td>0.189200</td>\n","      <td>1.499549</td>\n","    </tr>\n","    <tr>\n","      <td>192</td>\n","      <td>0.162200</td>\n","      <td>1.494696</td>\n","    </tr>\n","    <tr>\n","      <td>193</td>\n","      <td>0.189200</td>\n","      <td>1.497437</td>\n","    </tr>\n","    <tr>\n","      <td>194</td>\n","      <td>0.072700</td>\n","      <td>1.495646</td>\n","    </tr>\n","    <tr>\n","      <td>195</td>\n","      <td>0.055500</td>\n","      <td>1.495403</td>\n","    </tr>\n","    <tr>\n","      <td>196</td>\n","      <td>0.071700</td>\n","      <td>1.495829</td>\n","    </tr>\n","    <tr>\n","      <td>197</td>\n","      <td>0.195800</td>\n","      <td>1.494819</td>\n","    </tr>\n","    <tr>\n","      <td>198</td>\n","      <td>0.133700</td>\n","      <td>1.493407</td>\n","    </tr>\n","    <tr>\n","      <td>199</td>\n","      <td>0.236800</td>\n","      <td>1.492952</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.087900</td>\n","      <td>1.492652</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=11000, training_loss=0.6806682673692703, metrics={'train_runtime': 1838.2548, 'train_samples_per_second': 47.219, 'train_steps_per_second': 5.984, 'total_flos': 1.13415076970496e+16, 'train_loss': 0.6806682673692703, 'epoch': 200.0})"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","# Evaluate the model on the validation dataset\n","eval_results = trainer.evaluate()\n","\n","# Predict on the validation dataset\n","predictions, labels, _ = trainer.predict(val_dataset)\n","\n","# Get predicted labels (class with the highest probability)\n","predicted_labels_val = predictions.argmax(axis=1)\n","\n","# Calculate validation accuracy\n","validation_accuracy = accuracy_score(labels, predicted_labels_val)\n","\n","# Predict on the training dataset (you may need to pass train_dataset for this)\n","predictions_train, labels_train, _ = trainer.predict(train_dataset)\n","\n","# Get predicted labels for training (class with the highest probability)\n","predicted_labels_train = predictions_train.argmax(axis=1)\n","\n","# Calculate training accuracy\n","training_accuracy = accuracy_score(labels_train, predicted_labels_train)\n","\n","# Print the evaluation results (loss and other metrics for validation)\n","print(\"Evaluation Results:\", eval_results)\n","\n","# Print both training and validation accuracies in percentage format\n","print(f\"Training Accuracy: {training_accuracy * 100:.2f}%\")\n","print(f\"Validation Accuracy: {validation_accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"id":"AxH1XozyY1S6","executionInfo":{"status":"ok","timestamp":1735880456543,"user_tz":480,"elapsed":8947,"user":{"displayName":"wajeeha XYZ","userId":"02510792856806663883"}},"outputId":"595be757-b47e-4cde-92dc-7cebabd9cb57"},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation Results: {'eval_loss': 1.4926520586013794, 'eval_runtime': 0.7737, 'eval_samples_per_second': 63.332, 'eval_steps_per_second': 9.047, 'epoch': 200.0}\n","Training Accuracy: 99.77%\n","Validation Accuracy: 61.22%\n"]}]},{"cell_type":"code","source":["# Save the model to Google Drive\n","model.save_pretrained('/content/drive/My Drive/saved_model_gpt')\n","tokenizer.save_pretrained('/content/drive/My Drive/saved_model_gpt')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZKcBtJmTY3Tm","executionInfo":{"status":"ok","timestamp":1735880477971,"user_tz":480,"elapsed":3673,"user":{"displayName":"wajeeha XYZ","userId":"02510792856806663883"}},"outputId":"8e2ddb2c-d55a-430f-ffb6-1e6bf8ec5f07"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/My Drive/saved_model_gpt/tokenizer_config.json',\n"," '/content/drive/My Drive/saved_model_gpt/special_tokens_map.json',\n"," '/content/drive/My Drive/saved_model_gpt/vocab.json',\n"," '/content/drive/My Drive/saved_model_gpt/merges.txt',\n"," '/content/drive/My Drive/saved_model_gpt/added_tokens.json')"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["import torch\n","\n","# Function to test the model with a custom input\n","def test_model_with_input(input_text):\n","    # Ensure the model is on the correct device (GPU if available, otherwise CPU)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)  # Move the model to the correct device\n","\n","    # Tokenize the input text and move the tensors to the same device as the model\n","    inputs = tokenizer(input_text, truncation=True, padding='max_length', max_length=256, return_tensors=\"pt\")\n","    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move input tensors to the same device\n","\n","    # Set the model to evaluation mode\n","    model.eval()\n","\n","    # Pass the input through the model\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # Get the predicted class (the index of the highest logit)\n","    logits = outputs.logits\n","    predicted_class = torch.argmax(logits, dim=1).item()\n","\n","    # Map the predicted class ID to the label\n","    predicted_label = unique_labels[predicted_class]\n","\n","    # Print the input and the predicted label\n","    print(f\"Input Text: {input_text}\")\n","    print(f\"Predicted Label: {predicted_label}\")\n","\n","# Example test\n","input_example = \"I barely get enough sleep between classes and work, and I think it's taking a toll on my skin. My acne seems to be getting worse, and I have these dark circles under my eyes. I know I need to get more sleep, but it's hard to find the time. Do you have any advice?\"\n","#\"I have these red, itchy patches on my elbows and knees that won't go away. They're flaky and sometimes bleed when I scratch. I'm really self-conscious about them, especially in the summer.\"\n","#\"I noticed red circles on my feet after walking barefoot in the park.\"\n","#\"I feel embarrassed showing my skin; the hives make it look really bad.\"\n","#\"Doctor, I've noticed these small, red bumps on my wrists and elbows. They itch like crazy, and I'm starting to worry about bed bugs or something. Could it be something more serious?\"\n","test_model_with_input(input_example)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvXsdx-kmmvc","executionInfo":{"status":"ok","timestamp":1735882052361,"user_tz":480,"elapsed":591,"user":{"displayName":"wajeeha XYZ","userId":"02510792856806663883"}},"outputId":"83d937a1-c751-4516-8de9-304357e92332"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Text: I barely get enough sleep between classes and work, and I think it's taking a toll on my skin. My acne seems to be getting worse, and I have these dark circles under my eyes. I know I need to get more sleep, but it's hard to find the time. Do you have any advice?\n","Predicted Label: Acne\n"]}]}]}